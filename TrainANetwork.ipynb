{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf5e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SpectrogramDataset import SpectrogramDataset, MfccDataset, create_mfcc_dataset, create_dataframes, create_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c220ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_df, track_df, genre_df = create_dataframes(file_paths_path = 'data/all_data_paths.txt' , \n",
    "                                                     tracks_csv_path = 'data/fma_metadata/tracks.csv', \n",
    "                                                     genre_csv_path = 'data/fma_metadata/genres.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711ed6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load  data/fma_small/108/108925.mp3\n",
      "Failed to load  data/fma_small/099/099134.mp3\n",
      "Training dataset created.\n",
      "Validation dataset created.\n",
      "Failed to load  data/fma_small/133/133297.mp3\n",
      "Test dataset created.\n"
     ]
    }
   ],
   "source": [
    "# train_data, validation_data, test_data = create_dataset(file_path_df, track_df, genre_df, test_percentage = .10, validation_percentage = .10)\n",
    "\n",
    "train_data, validation_data, test_data = create_mfcc_dataset(file_path_df, track_df, genre_df, test_percentage = .10, validation_percentage = .10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6343b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels= 1, kernel_size = 3, stride = 1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, stride = 1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "#         self.fc1 = nn.Linear(9280, 928)\n",
    "#         self.fc2 = nn.Linear(928, 256)\n",
    "        self.fc3 = nn.Linear(1869, 16)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:,None, :, :]\n",
    "        x = self.ReLU(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.ReLU(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "#         x = self.ReLU(self.conv3(x))\n",
    "#         x = self.maxpool(x)\n",
    "        x = self.flatten(x)  \n",
    "#         print(x.shape)\n",
    "#         x = self.ReLU(self.fc1(x))\n",
    "#         x = self.ReLU(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def evaluate(model, validation_loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    for inputs, labels in validation_loader:\n",
    "        outputs = model(inputs)\n",
    "        val_loss += criterion(outputs, labels)\n",
    "        _, true_labels = torch.max(labels, dim = 1)\n",
    "        \n",
    "        _, pred_labels = torch.max(outputs, dim = 1)\n",
    "                \n",
    "        val_acc += torch.tensor(torch.sum(pred_labels == true_labels).item()/len(pred_labels))\n",
    "    \n",
    "    val_loss = val_loss/len(validation_loader)\n",
    "    val_acc = val_acc/len(validation_loader)\n",
    "        \n",
    "    return val_loss.detach().item(), val_acc.item()\n",
    "        \n",
    "        \n",
    "\n",
    "def train_NN(model, train_loader, validation_loader, learning_rate:float, epochs:int=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            train_loss = criterion(outputs, labels)\n",
    "\n",
    "            train_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += train_loss.item()\n",
    "\n",
    "        loss = loss/len(train_loader)\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, validation_loader, criterion)\n",
    "\n",
    "        print(\"epoch: {}/{}, train loss = {:.6f}, val loss = {:.6f}, val acc = {:.6f}\".format(epoch+1, epochs, loss, val_loss, val_acc))\n",
    "\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7016f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/50, train loss = 2.717969, val loss = 2.718865, val acc = 0.112500\n",
      "epoch: 2/50, train loss = 2.713457, val loss = 2.713205, val acc = 0.111111\n",
      "epoch: 3/50, train loss = 2.712883, val loss = 2.712090, val acc = 0.112500\n",
      "epoch: 4/50, train loss = 2.711627, val loss = 2.716932, val acc = 0.112500\n",
      "epoch: 5/50, train loss = 2.712237, val loss = 2.711705, val acc = 0.112500\n",
      "epoch: 6/50, train loss = 2.711093, val loss = 2.727983, val acc = 0.112500\n",
      "epoch: 7/50, train loss = 2.710013, val loss = 2.718814, val acc = 0.112500\n",
      "epoch: 8/50, train loss = 2.710401, val loss = 2.710261, val acc = 0.112500\n",
      "epoch: 9/50, train loss = 2.709114, val loss = 2.721027, val acc = 0.112500\n",
      "epoch: 10/50, train loss = 2.708561, val loss = 2.709153, val acc = 0.127778\n",
      "epoch: 11/50, train loss = 2.707309, val loss = 2.713187, val acc = 0.115278\n",
      "epoch: 12/50, train loss = 2.707645, val loss = 2.710267, val acc = 0.116667\n",
      "epoch: 13/50, train loss = 2.705996, val loss = 2.706559, val acc = 0.131944\n",
      "epoch: 14/50, train loss = 2.707058, val loss = 2.707092, val acc = 0.133333\n",
      "epoch: 15/50, train loss = 2.705075, val loss = 2.721055, val acc = 0.112500\n",
      "epoch: 16/50, train loss = 2.705844, val loss = 2.705967, val acc = 0.152778\n",
      "epoch: 17/50, train loss = 2.703031, val loss = 2.705025, val acc = 0.134722\n",
      "epoch: 18/50, train loss = 2.702530, val loss = 2.706712, val acc = 0.172222\n",
      "epoch: 19/50, train loss = 2.701508, val loss = 2.706827, val acc = 0.131944\n",
      "epoch: 20/50, train loss = 2.702672, val loss = 2.705222, val acc = 0.129167\n",
      "epoch: 21/50, train loss = 2.699932, val loss = 2.710047, val acc = 0.125000\n",
      "epoch: 22/50, train loss = 2.698395, val loss = 2.702826, val acc = 0.141667\n",
      "epoch: 23/50, train loss = 2.699470, val loss = 2.700103, val acc = 0.150000\n",
      "epoch: 24/50, train loss = 2.698137, val loss = 2.699289, val acc = 0.163889\n",
      "epoch: 25/50, train loss = 2.696055, val loss = 2.702601, val acc = 0.169444\n",
      "epoch: 26/50, train loss = 2.695185, val loss = 2.699852, val acc = 0.161111\n",
      "epoch: 27/50, train loss = 2.695593, val loss = 2.696353, val acc = 0.165278\n",
      "epoch: 28/50, train loss = 2.693549, val loss = 2.697053, val acc = 0.169444\n",
      "epoch: 29/50, train loss = 2.691998, val loss = 2.695948, val acc = 0.156944\n",
      "epoch: 30/50, train loss = 2.691148, val loss = 2.694106, val acc = 0.170833\n",
      "epoch: 31/50, train loss = 2.687457, val loss = 2.690885, val acc = 0.163889\n",
      "epoch: 32/50, train loss = 2.687555, val loss = 2.691579, val acc = 0.173611\n",
      "epoch: 33/50, train loss = 2.683766, val loss = 2.694819, val acc = 0.161111\n",
      "epoch: 34/50, train loss = 2.681546, val loss = 2.688241, val acc = 0.173611\n",
      "epoch: 35/50, train loss = 2.679259, val loss = 2.681621, val acc = 0.170833\n",
      "epoch: 36/50, train loss = 2.676320, val loss = 2.683902, val acc = 0.183333\n",
      "epoch: 37/50, train loss = 2.672042, val loss = 2.674650, val acc = 0.181944\n",
      "epoch: 38/50, train loss = 2.669221, val loss = 2.672634, val acc = 0.201389\n",
      "epoch: 39/50, train loss = 2.665664, val loss = 2.669317, val acc = 0.190278\n",
      "epoch: 40/50, train loss = 2.660704, val loss = 2.668081, val acc = 0.190278\n",
      "epoch: 41/50, train loss = 2.657468, val loss = 2.660986, val acc = 0.195833\n",
      "epoch: 42/50, train loss = 2.653454, val loss = 2.661716, val acc = 0.190278\n",
      "epoch: 43/50, train loss = 2.652372, val loss = 2.655177, val acc = 0.202778\n",
      "epoch: 44/50, train loss = 2.648519, val loss = 2.654578, val acc = 0.209722\n",
      "epoch: 45/50, train loss = 2.646666, val loss = 2.648459, val acc = 0.216667\n",
      "epoch: 46/50, train loss = 2.643730, val loss = 2.648978, val acc = 0.218056\n",
      "epoch: 47/50, train loss = 2.639808, val loss = 2.646503, val acc = 0.219444\n",
      "epoch: 48/50, train loss = 2.636608, val loss = 2.645523, val acc = 0.223611\n",
      "epoch: 49/50, train loss = 2.635847, val loss = 2.651268, val acc = 0.216667\n",
      "epoch: 50/50, train loss = 2.634176, val loss = 2.644190, val acc = 0.227778\n"
     ]
    }
   ],
   "source": [
    "# print(train_data.shape)\n",
    "# train_data_tensor = torch.tensor(train_data)\n",
    "# print(train_data_tensor.shape)\n",
    "model = NN()\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size = 10, shuffle = False)\n",
    "train_NN(model, train_loader, validation_loader, learning_rate = 1e-3, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2dd3bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6441901433799 0.22777777910232544\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "test_loader = torch.utils.data.DataLoader(validation_data, batch_size = 1, shuffle = False)\n",
    "loss, acc = evaluate(model, test_loader, criterion)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309188bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
